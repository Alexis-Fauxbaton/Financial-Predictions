{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datahandler import *\n",
    "from playground import *\n",
    "from gui import *\n",
    "from torchutils import *\n",
    "from models import *\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15\n",
    "agent_horizon = 7\n",
    "crossover_horizon = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "if minute:\n",
    "    data = pd.read_csv(\"BTCUSDT_15m.csv\")\n",
    "    data.drop([\"Unnamed: 0\", \"Close Unix\"], axis=1, inplace=True)\n",
    "\n",
    "else:\n",
    "    if not os.path.exists('./BTCUSDT_DB.csv'):\n",
    "        data = pd.read_csv(\"BTCUSDT_1m.csv\")\n",
    "        data = get_dollar_bars(data)\n",
    "        data.to_csv('./BTCUSDT_DB.csv', sep=',')\n",
    "    else:\n",
    "        data = pd.read_csv('./BTCUSDT_DB.csv', sep=',')\n",
    "\n",
    "handler = NewDataHandler(dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data['Target'] = handler.data['Label']\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data['Crossover'] = handler.data['Crossover']\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[[f'Kendall_{k1}', f'Kendall_{k2}']] = handler.data[[f'Kendall_{k1}', f'Kendall_{k2}']]\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[[f'Kendall_{k1}', f'Kendall_{k2}']] = handler.data[[f'Kendall_{k1}', f'Kendall_{k2}']]\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[f'MA{ma1} Var'], predict_data[f'MA{ma2} Var'] = handler.data[f'MA{ma1} Var'], handler.data[f'MA{ma2} Var']\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[f'MA{ma1} Var'], predict_data[f'MA{ma2} Var'] = handler.data[f'MA{ma1} Var'], handler.data[f'MA{ma2} Var']\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[dummies.columns] = handler.data[dummies.columns]\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[dummies.columns] = handler.data[dummies.columns]\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[dummies.columns] = handler.data[dummies.columns]\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[f'MA{ma1} UP'], predict_data[f'MA{ma2} UP'] = (handler.data[f'MA{ma1}'] > handler.data[f'MA{ma2}']) * 1, (handler.data[f'MA{ma1}'] < handler.data[f'MA{ma2}']) * 1\n",
      "C:\\Users\\Alexis\\AppData\\Local\\Temp\\ipykernel_25968\\2782349095.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_data[f'MA{ma1} UP'], predict_data[f'MA{ma2} UP'] = (handler.data[f'MA{ma1}'] > handler.data[f'MA{ma2}']) * 1, (handler.data[f'MA{ma1}'] < handler.data[f'MA{ma2}']) * 1\n"
     ]
    }
   ],
   "source": [
    "ma1, ma2 = 5, 10\n",
    "k1, k2 = 10, 30\n",
    "handler.data = ma_crossover_labelling(handler.data, ma1, ma2, crossover_horizon)\n",
    "handler.data.rename({'Label': 'Crossover'}, axis=1, inplace=True)\n",
    "dummies = pd.get_dummies(handler.data['Crossover'], prefix='Crossover') * 1\n",
    "# handler.data.drop('Crossover', axis=1, inplace=True)\n",
    "handler.data = pd.concat([handler.data, dummies], axis=1)\n",
    "handler.data = triple_barrier_labelling(handler.data, upper_barrier=1.004, lower_barrier=0.996, time_limit=agent_horizon)\n",
    "handler.data = add_kendall_tau(handler.data, k1)\n",
    "handler.data = add_kendall_tau(handler.data, k2)\n",
    "\n",
    "handler.add_indicators([Indicators.RSI, Indicators.MACD,\n",
    "                       Indicators.ADX, Indicators.OBV, Indicators.NTRADES])\n",
    "\n",
    "handler.create_var_indicator([Indicators.RSI, Indicators.MACD, Indicators.ADX, Indicators.OBV, Indicators.LOG_RET, Indicators.NTRADES])\n",
    "\n",
    "\n",
    "# display(handler.data[handler.data.isnull().any(axis=1)]) # Displaying all the rows that contain missing values to see if they are spread across the dataframe\n",
    "\n",
    "handler.data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# handler.standardize_data()\n",
    "\n",
    "handler.data.dropna(axis=0, inplace=True)\n",
    "\n",
    "handler.create_predict_data()\n",
    "\n",
    "predict_data = handler.predict_data\n",
    "predict_data['Target'] = handler.data['Label']\n",
    "predict_data['Crossover'] = handler.data['Crossover']\n",
    "predict_data[[f'Kendall_{k1}', f'Kendall_{k2}']] = handler.data[[f'Kendall_{k1}', f'Kendall_{k2}']]\n",
    "predict_data[f'MA{ma1} Var'], predict_data[f'MA{ma2} Var'] = handler.data[f'MA{ma1} Var'], handler.data[f'MA{ma2} Var']\n",
    "predict_data[dummies.columns] = handler.data[dummies.columns]\n",
    "predict_data[f'MA{ma1} UP'], predict_data[f'MA{ma2} UP'] = (handler.data[f'MA{ma1}'] > handler.data[f'MA{ma2}']) * 1, (handler.data[f'MA{ma1}'] < handler.data[f'MA{ma2}']) * 1\n",
    "predict_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17398,  6357,  6848],\n",
       "       [52868, 27474, 51800],\n",
       "       [ 7271,  6612, 16728]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix(handler.data['Crossover'], handler.data['Label'], labels=[-1, 0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossover_lstm_model = torch.load(\n",
    "    f'./models/crossover_{seq_length}_to_{crossover_horizon}.pt').to(device)\n",
    "lstm_model = torch.load(f'./models/agent_{seq_length}_to_{agent_horizon}.pt').to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_columns = predict_data.drop(['Crossover'], axis=1).columns\n",
    "crossover_columns = predict_data.drop(dummies.columns, axis=1).columns\n",
    "ma_dataset = TSDataset(\n",
    "    predict_data[crossover_columns], seq_length, 'Crossover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unix', 'RSI Var', 'MACD Var', 'MACD_H Var', 'ADX14 Var', '-DM Var',\n",
       "       '+DM Var', 'OBV Var', 'NTrades Var', 'RSI_30-', 'RSI_BTW', 'RSI_70+',\n",
       "       'LOG_RET', 'Target', 'Kendall_10', 'Kendall_30', 'MA5 Var', 'MA10 Var',\n",
       "       'Crossover_-1', 'Crossover_0', 'Crossover_1', 'MA5 UP', 'MA10 UP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unix', 'RSI Var', 'MACD Var', 'MACD_H Var', 'ADX14 Var', '-DM Var',\n",
       "       '+DM Var', 'OBV Var', 'NTrades Var', 'RSI_30-', 'RSI_BTW', 'RSI_70+',\n",
       "       'LOG_RET', 'Target', 'Crossover', 'Kendall_10', 'Kendall_30', 'MA5 Var',\n",
       "       'MA10 Var', 'MA5 UP', 'MA10 UP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossover_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing real crossover predictions by infered crossover predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7482168810547168 || Loss : 0.795134961605072\n",
      "Confusion matrix : \n",
      "[[1.73730e+04 1.32240e+04 3.00000e+00]\n",
      " [1.23580e+04 1.09383e+05 1.03920e+04]\n",
      " [1.00000e+00 1.27020e+04 1.79050e+04]]\n"
     ]
    }
   ],
   "source": [
    "predicted_crossover_outputs, predicted_crossover_targets = eval_lstm(crossover_lstm_model, ma_dataset, 64, crossover_lstm_model.num_layers, crossover_lstm_model.hidden_size, device)\n",
    "\n",
    "predicted_crossover_outputs = pd.Series(\n",
    "    (torch.argmax(predicted_crossover_outputs, axis=-1) - 1).cpu()).shift(seq_length)\n",
    "\n",
    "crossover_prediction_dummies = pd.get_dummies(\n",
    "    predicted_crossover_outputs, prefix='Crossover') * 1\n",
    "\n",
    "transition_predict_data = predict_data.copy()\n",
    "transition_predict_data['Crossover'] = predicted_crossover_outputs\n",
    "transition_predict_data[dummies.columns] = crossover_prediction_dummies\n",
    "transition_predict_data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[11898,  6364, 11466],\n",
       "        [54653, 27891, 52757],\n",
       "        [10972,  6186, 11139]], dtype=int64),\n",
       " 0.2634306818534496)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "m = confusion_matrix(transition_predict_data['Crossover'], transition_predict_data['Target'], labels=[-1, 0, 1])\n",
    "accuracy = (m.diagonal().sum()) / m.sum()\n",
    "m, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[17398,  6357,  6848],\n",
       "        [52868, 27474, 51800],\n",
       "        [ 7271,  6612, 16728]], dtype=int64),\n",
       " 0.31858333850514076)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = confusion_matrix(predict_data['Crossover'], predict_data['Target'], labels=[-1, 0, 1])\n",
    "accuracy = (m.diagonal().sum()) / m.sum()\n",
    "m, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TSDataset(transition_predict_data[trade_columns], seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    1.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax_dummy = np.argmax(transition_predict_data[dummies.columns].values, axis=1)\n",
    "argmax_values = pd.Series(argmax_dummy) - 1\n",
    "(transition_predict_data['Crossover'].to_list() == argmax_values).value_counts() / argmax_values.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting regular model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = range(round(0.7 * len(dataset)), round(0.9 * len(dataset)), int(seq_length / 5))\n",
    "val_indices = range(round(0.9 * len(dataset)), len(dataset) - seq_length)\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_indices)\n",
    "# Validation set will be training set for Meta Labelling\n",
    "val_set = torch.utils.data.Subset(dataset, val_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "-1.0    5830\n",
       " 1.0    5534\n",
       " 0.0    1524\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.45235878336436997, 0.11824953445065177, 0.42939168218497825]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution = (transition_predict_data.loc[train_set.indices, 'Target'].value_counts(\n",
    ") / transition_predict_data.loc[train_set.indices, 'Target'].shape[0]).sort_index().to_list()\n",
    "display(transition_predict_data.loc[train_set.indices, 'Target'].value_counts())\n",
    "label_distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       " 0.0    7222\n",
       "-1.0    6095\n",
       " 1.0    5999\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.31554151998343344, 0.37388693311244564, 0.3105715469041209]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label_distribution = (transition_predict_data.loc[val_set.indices, 'Target'].value_counts(\n",
    ") / transition_predict_data.loc[val_set.indices, 'Target'].shape[0]).sort_index().to_list()\n",
    "display(transition_predict_data.loc[val_set.indices, 'Target'].value_counts())\n",
    "val_label_distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2106346483704975, 8.456692913385826, 2.3288760390314422]\n"
     ]
    }
   ],
   "source": [
    "class_weights = [1 / p for p in label_distribution]\n",
    "print(class_weights)\n",
    "weights = [class_weights[torch.argmax(label)] for _, label in train_set]\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=weights, num_samples=len(train_set), replacement=True)\n",
    "# train_sampler = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = lstm_model.input_size\n",
    "hidden_size = lstm_model.hidden_size\n",
    "batch_size = 64\n",
    "num_layers = lstm_model.num_layers\n",
    "output_size = transition_predict_data['Target'].unique().size\n",
    "new_lstm_model = LSTMModel(input_size, hidden_size,\n",
    "                       num_layers, output_size).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_lstm(lstm_model, val_set, len(val_set), num_layers, hidden_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15 -- [4848/12888 (100.0%)]\tLoss: 1.1057418838585957\tAccuracy: 0.334\tTime taken: 11.59375\tValidation Loss: 1.1004434823989868 || Validation Accuracy: 0.326\n",
      "Epoch: 2/15 -- [4848/12888 (100.0%)]\tLoss: 1.099307108633589\tAccuracy: 0.331\tTime taken: 11.59375\tValidation Loss: 1.0996109247207642 || Validation Accuracy: 0.321\n",
      "Epoch: 3/15 -- [4848/12888 (100.0%)]\tLoss: 1.0981216306733612\tAccuracy: 0.335\tTime taken: 11.59375\tValidation Loss: 1.098779320716858 || Validation Accuracy: 0.331\n",
      "Epoch: 4/15 -- [4848/12888 (100.0%)]\tLoss: 1.0971987200255442\tAccuracy: 0.354\tTime taken: 12.125\tValidation Loss: 1.0965440273284912 || Validation Accuracy: 0.371\n",
      "Epoch: 5/15 -- [4848/12888 (100.0%)]\tLoss: 1.0975265497028237\tAccuracy: 0.354\tTime taken: 11.34375\tValidation Loss: 1.0978562831878662 || Validation Accuracy: 0.360\n",
      "Epoch: 6/15 -- [4848/12888 (100.0%)]\tLoss: 1.0961679438553233\tAccuracy: 0.361\tTime taken: 11.53125\tValidation Loss: 1.0972752571105957 || Validation Accuracy: 0.359\n",
      "Epoch: 7/15 -- [4848/12888 (100.0%)]\tLoss: 1.0921372966010972\tAccuracy: 0.375\tTime taken: 11.703125\tValidation Loss: 1.093540072441101 || Validation Accuracy: 0.379\n",
      "Epoch: 8/15 -- [4848/12888 (100.0%)]\tLoss: 1.0950072832626871\tAccuracy: 0.358\tTime taken: 11.46875\tValidation Loss: 1.0921281576156616 || Validation Accuracy: 0.392\n",
      "Epoch: 9/15 -- [4848/12888 (100.0%)]\tLoss: 1.090131932556039\tAccuracy: 0.379\tTime taken: 11.4375\tValidation Loss: 1.1000254154205322 || Validation Accuracy: 0.338\n",
      "Epoch: 10/15 -- [4848/12888 (100.0%)]\tLoss: 1.0923701265070698\tAccuracy: 0.375\tTime taken: 11.453125\tValidation Loss: 1.0953303575515747 || Validation Accuracy: 0.367\n",
      "Epoch: 11/15 -- [4848/12888 (100.0%)]\tLoss: 1.0906509639012931\tAccuracy: 0.381\tTime taken: 11.453125\tValidation Loss: 1.098096251487732 || Validation Accuracy: 0.352\n",
      "Epoch: 12/15 -- [4848/12888 (100.0%)]\tLoss: 1.0868557597150896\tAccuracy: 0.390\tTime taken: 11.515625\tValidation Loss: 1.0946846008300781 || Validation Accuracy: 0.371\n",
      "Epoch: 13/15 -- [4848/12888 (100.0%)]\tLoss: 1.0865245609000178\tAccuracy: 0.386\tTime taken: 11.5\tValidation Loss: 1.0937402248382568 || Validation Accuracy: 0.379\n",
      "Epoch: 14/15 -- [4848/12888 (100.0%)]\tLoss: 1.0852835715407192\tAccuracy: 0.393\tTime taken: 11.4375\tValidation Loss: 1.102541208267212 || Validation Accuracy: 0.343\n",
      "Epoch: 15/15 -- [4848/12888 (100.0%)]\tLoss: 1.0851137118764442\tAccuracy: 0.391\tTime taken: 11.78125\tValidation Loss: 1.0989130735397339 || Validation Accuracy: 0.357\n",
      "Best accuracy : 0.3923172499482295 || Best confusion matrix : \n",
      " [[ 604. 3962. 1526.]\n",
      " [ 507. 5276. 1451.]\n",
      " [ 591. 3701. 1698.]]\n",
      "Last confusion matrix : \n",
      " [[1011. 2071. 3010.]\n",
      " [1076. 2869. 3289.]\n",
      " [1005. 1962. 3023.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.1057418838585957,\n",
       "  1.099307108633589,\n",
       "  1.0981216306733612,\n",
       "  1.0971987200255442,\n",
       "  1.0975265497028237,\n",
       "  1.0961679438553233,\n",
       "  1.0921372966010972,\n",
       "  1.0950072832626871,\n",
       "  1.090131932556039,\n",
       "  1.0923701265070698,\n",
       "  1.0906509639012931,\n",
       "  1.0868557597150896,\n",
       "  1.0865245609000178,\n",
       "  1.0852835715407192,\n",
       "  1.0851137118764442],\n",
       " [0.3335661080074488,\n",
       "  0.33123836126629425,\n",
       "  0.33511793916821847,\n",
       "  0.353584729981378,\n",
       "  0.353584729981378,\n",
       "  0.3612662942271881,\n",
       "  0.37468963376784603,\n",
       "  0.3578522656734947,\n",
       "  0.3787243947858473,\n",
       "  0.3754655493482309,\n",
       "  0.38050900062073245,\n",
       "  0.39020794537554315,\n",
       "  0.3861731843575419,\n",
       "  0.3929236499068901,\n",
       "  0.3914494103041589],\n",
       " [tensor(1.1004, device='cuda:0'),\n",
       "  tensor(1.0996, device='cuda:0'),\n",
       "  tensor(1.0988, device='cuda:0'),\n",
       "  tensor(1.0965, device='cuda:0'),\n",
       "  tensor(1.0979, device='cuda:0'),\n",
       "  tensor(1.0973, device='cuda:0'),\n",
       "  tensor(1.0935, device='cuda:0'),\n",
       "  tensor(1.0921, device='cuda:0'),\n",
       "  tensor(1.1000, device='cuda:0'),\n",
       "  tensor(1.0953, device='cuda:0'),\n",
       "  tensor(1.0981, device='cuda:0'),\n",
       "  tensor(1.0947, device='cuda:0'),\n",
       "  tensor(1.0937, device='cuda:0'),\n",
       "  tensor(1.1025, device='cuda:0'),\n",
       "  tensor(1.0989, device='cuda:0')],\n",
       " [0.32605094222406295,\n",
       "  0.32113273969766,\n",
       "  0.3306585214330089,\n",
       "  0.37088424104369433,\n",
       "  0.3595464899565127,\n",
       "  0.3592876371919652,\n",
       "  0.3794263822737627,\n",
       "  0.3923172499482295,\n",
       "  0.33754400496997305,\n",
       "  0.36663905570511496,\n",
       "  0.35157382480844895,\n",
       "  0.37103955270242284,\n",
       "  0.3792193000621247,\n",
       "  0.3426692897080141,\n",
       "  0.3573721267343135],\n",
       " array([[ 604., 3962., 1526.],\n",
       "        [ 507., 5276., 1451.],\n",
       "        [ 591., 3701., 1698.]]),\n",
       " 0.3923172499482295)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.Tensor(label_distribution).to(device)\n",
    "train_lstm(lstm_model, train_set, val_set, 15, 0.0001,\n",
    "           batch_size, lstm_model.num_layers, lstm_model.hidden_size, device, train_sampler, class_weights=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating sampler mid training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m class_weights \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(class_weights)\n\u001b[1;32m----> 3\u001b[0m weights \u001b[39m=\u001b[39m [class_weights[torch\u001b[39m.\u001b[39margmax(label)] \u001b[39mfor\u001b[39;00m _, label \u001b[39min\u001b[39;00m train_set]\n\u001b[0;32m      4\u001b[0m train_sampler \u001b[39m=\u001b[39m WeightedRandomSampler(\n\u001b[0;32m      5\u001b[0m     weights\u001b[39m=\u001b[39mweights, num_samples\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_set), replacement\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m class_weights \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(class_weights)\n\u001b[1;32m----> 3\u001b[0m weights \u001b[39m=\u001b[39m [class_weights[torch\u001b[39m.\u001b[39;49margmax(label)] \u001b[39mfor\u001b[39;00m _, label \u001b[39min\u001b[39;00m train_set]\n\u001b[0;32m      4\u001b[0m train_sampler \u001b[39m=\u001b[39m WeightedRandomSampler(\n\u001b[0;32m      5\u001b[0m     weights\u001b[39m=\u001b[39mweights, num_samples\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_set), replacement\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "class_weights = []\n",
    "print(class_weights)\n",
    "weights = [class_weights[torch.argmax(label)] for _, label in train_set]\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=weights, num_samples=len(train_set), replacement=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
