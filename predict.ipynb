{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BTC-USD_SIGNALS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Variation</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_H</th>\n",
       "      <th>-DM</th>\n",
       "      <th>+DM</th>\n",
       "      <th>ADX14</th>\n",
       "      <th>Confirmation Time</th>\n",
       "      <th>Transactions</th>\n",
       "      <th>Miners Revenue</th>\n",
       "      <th>FnG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>10237.299805</td>\n",
       "      <td>10288.799805</td>\n",
       "      <td>8812.280273</td>\n",
       "      <td>9170.540039</td>\n",
       "      <td>-0.292146</td>\n",
       "      <td>-2.712213</td>\n",
       "      <td>-1.043523</td>\n",
       "      <td>-1.108359</td>\n",
       "      <td>-0.475907</td>\n",
       "      <td>1.319161</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.521410</td>\n",
       "      <td>1.144750</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-0.583209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>9142.280273</td>\n",
       "      <td>9142.280273</td>\n",
       "      <td>7796.490234</td>\n",
       "      <td>8830.750000</td>\n",
       "      <td>-0.152676</td>\n",
       "      <td>-1.011481</td>\n",
       "      <td>-1.204172</td>\n",
       "      <td>-1.202571</td>\n",
       "      <td>-0.654534</td>\n",
       "      <td>1.402518</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.393472</td>\n",
       "      <td>1.144750</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-1.244407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>8852.120117</td>\n",
       "      <td>9430.750000</td>\n",
       "      <td>8251.629883</td>\n",
       "      <td>9174.910156</td>\n",
       "      <td>-0.427993</td>\n",
       "      <td>0.955604</td>\n",
       "      <td>-1.632900</td>\n",
       "      <td>-1.235294</td>\n",
       "      <td>-0.618233</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>0.287977</td>\n",
       "      <td>0.409785</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-0.142410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>9175.700195</td>\n",
       "      <td>9334.870117</td>\n",
       "      <td>8031.220215</td>\n",
       "      <td>8277.009766</td>\n",
       "      <td>-0.437581</td>\n",
       "      <td>-2.584947</td>\n",
       "      <td>-1.626830</td>\n",
       "      <td>-1.320379</td>\n",
       "      <td>-0.741794</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.427573</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-0.847688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>8270.540039</td>\n",
       "      <td>8364.839844</td>\n",
       "      <td>6756.680176</td>\n",
       "      <td>6955.270020</td>\n",
       "      <td>-0.326118</td>\n",
       "      <td>-4.184569</td>\n",
       "      <td>-1.814295</td>\n",
       "      <td>-1.479953</td>\n",
       "      <td>-1.057732</td>\n",
       "      <td>1.828350</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.462483</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-1.420727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>2022-09-04</td>\n",
       "      <td>19832.470703</td>\n",
       "      <td>19999.689453</td>\n",
       "      <td>19636.816406</td>\n",
       "      <td>19986.712891</td>\n",
       "      <td>0.478227</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>-1.113287</td>\n",
       "      <td>-0.844382</td>\n",
       "      <td>-0.254845</td>\n",
       "      <td>-0.167922</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.699748</td>\n",
       "      <td>-1.063067</td>\n",
       "      <td>-0.648298</td>\n",
       "      <td>-0.307825</td>\n",
       "      <td>-1.024008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>19988.789063</td>\n",
       "      <td>20031.160156</td>\n",
       "      <td>19673.046875</td>\n",
       "      <td>19812.371094</td>\n",
       "      <td>0.658018</td>\n",
       "      <td>-0.278482</td>\n",
       "      <td>-1.133325</td>\n",
       "      <td>-0.831832</td>\n",
       "      <td>-0.166540</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>-0.256709</td>\n",
       "      <td>0.691814</td>\n",
       "      <td>-0.742360</td>\n",
       "      <td>-0.648298</td>\n",
       "      <td>-0.307825</td>\n",
       "      <td>-0.891768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>19817.724609</td>\n",
       "      <td>20155.269531</td>\n",
       "      <td>18800.171875</td>\n",
       "      <td>18837.667969</td>\n",
       "      <td>1.393318</td>\n",
       "      <td>-1.325705</td>\n",
       "      <td>-1.548973</td>\n",
       "      <td>-0.892284</td>\n",
       "      <td>-0.308652</td>\n",
       "      <td>1.167318</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.776149</td>\n",
       "      <td>-0.742360</td>\n",
       "      <td>-0.165203</td>\n",
       "      <td>-0.359746</td>\n",
       "      <td>-0.935848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>2022-09-07</td>\n",
       "      <td>18837.683594</td>\n",
       "      <td>19427.171875</td>\n",
       "      <td>18644.466797</td>\n",
       "      <td>19290.324219</td>\n",
       "      <td>0.981876</td>\n",
       "      <td>0.568952</td>\n",
       "      <td>-1.180050</td>\n",
       "      <td>-0.893278</td>\n",
       "      <td>-0.249058</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.881401</td>\n",
       "      <td>-0.742360</td>\n",
       "      <td>-0.165203</td>\n",
       "      <td>-0.359746</td>\n",
       "      <td>-0.847688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>2022-09-08</td>\n",
       "      <td>19289.941406</td>\n",
       "      <td>19417.351563</td>\n",
       "      <td>19076.714844</td>\n",
       "      <td>19329.833984</td>\n",
       "      <td>0.828407</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-1.279608</td>\n",
       "      <td>-0.881242</td>\n",
       "      <td>-0.163410</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.910845</td>\n",
       "      <td>-0.653287</td>\n",
       "      <td>-0.165203</td>\n",
       "      <td>-0.359746</td>\n",
       "      <td>-1.024008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1684 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date          Open  ...  Miners Revenue       FnG\n",
       "0     2018-02-01  10237.299805  ...       -0.186892 -0.583209\n",
       "1     2018-02-02   9142.280273  ...       -0.186892 -1.244407\n",
       "2     2018-02-03   8852.120117  ...       -0.186892 -0.142410\n",
       "3     2018-02-04   9175.700195  ...       -0.264496 -0.847688\n",
       "4     2018-02-05   8270.540039  ...       -0.264496 -1.420727\n",
       "...          ...           ...  ...             ...       ...\n",
       "1679  2022-09-04  19832.470703  ...       -0.307825 -1.024008\n",
       "1680  2022-09-05  19988.789063  ...       -0.307825 -0.891768\n",
       "1681  2022-09-06  19817.724609  ...       -0.359746 -0.935848\n",
       "1682  2022-09-07  18837.683594  ...       -0.359746 -0.847688\n",
       "1683  2022-09-08  19289.941406  ...       -0.359746 -1.024008\n",
       "\n",
       "[1684 rows x 17 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = data.copy().drop([\"Open\",\"Close\",\"High\",\"Low\"],axis=1)\n",
    "max_days = 5\n",
    "target_range = 2\n",
    "for i in range(1,max_days):#2jours\n",
    "    predict_data[[\"Variation-{}\".format(i),\"Vol-{}\".format(i),\"RSI-{}\".format(i),\"MACD-{}\".format(i),\"MACD_H-{}\".format(i),\"CONF-{}\".format(i),\"TRANS-{}\".format(i),\"REV-{}\".format(i),\"FnG-{}\".format(i)]] = data[[\"Variation\",\"Volume\",\"RSI\",\"MACD\",\"MACD_H\",\"Confirmation Time\",\"Transactions\",\"Miners Revenue\",\"FnG\"]].shift(i)\n",
    "    #predict_data[[\"Variation-{}\".format(i),\"Vol-{}\".format(i),\"RSI-{}\".format(i),\"MACD-{}\".format(i),\"MACD_H-{}\".format(i),\"CONF-{}\".format(i),\"TRANS-{}\".format(i),\"REV-{}\".format(i),\"FnG-{}\".format(i), \"ADX-{}\".format(i), \"+DM-{}\".format(i), \"-DM-{}\".format(i)]] = data[[\"Variation\",\"Volume\",\"RSI\",\"MACD\",\"MACD_H\",\"Confirmation Time\",\"Transactions\",\"Miners Revenue\",\"FnG\",\"ADX14\",\"+DM\",\"-DM\"]].shift(i)\n",
    "    #predict_data[[\"Variation-{}\".format(i),\"Vol-{}\".format(i),\"RSI-{}\".format(i),\"MACD-{}\".format(i),\"MACD_H-{}\".format(i)]] = data[[\"Variation\",\"Volume\",\"RSI\",\"MACD\",\"MACD_H\"]].shift(i)\n",
    "#predict_data[\"Target\"] = (data[\"Variation\"].shift(-1) >= 0)\n",
    "predict_data[\"Target\"] = (data[\"Close\"].shift(-target_range) - data[\"Close\"] >= 0)\n",
    "predict_data[\"Target\"] = np.where(predict_data[\"Target\"] == True, 1, 0)\n",
    "predict_data.dropna(inplace=True)\n",
    "predict_data.reset_index(inplace=True,drop=True)\n",
    "predict_data = predict_data[0:len(predict_data)-target_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = predict_data[[i % int(max_days / 2) == 0 for i in range(len(predict_data))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Variation</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_H</th>\n",
       "      <th>-DM</th>\n",
       "      <th>+DM</th>\n",
       "      <th>ADX14</th>\n",
       "      <th>Confirmation Time</th>\n",
       "      <th>Transactions</th>\n",
       "      <th>Miners Revenue</th>\n",
       "      <th>FnG</th>\n",
       "      <th>Variation-1</th>\n",
       "      <th>Vol-1</th>\n",
       "      <th>RSI-1</th>\n",
       "      <th>MACD-1</th>\n",
       "      <th>MACD_H-1</th>\n",
       "      <th>CONF-1</th>\n",
       "      <th>TRANS-1</th>\n",
       "      <th>REV-1</th>\n",
       "      <th>FnG-1</th>\n",
       "      <th>Variation-2</th>\n",
       "      <th>Vol-2</th>\n",
       "      <th>RSI-2</th>\n",
       "      <th>MACD-2</th>\n",
       "      <th>MACD_H-2</th>\n",
       "      <th>CONF-2</th>\n",
       "      <th>TRANS-2</th>\n",
       "      <th>REV-2</th>\n",
       "      <th>FnG-2</th>\n",
       "      <th>Variation-3</th>\n",
       "      <th>Vol-3</th>\n",
       "      <th>RSI-3</th>\n",
       "      <th>MACD-3</th>\n",
       "      <th>MACD_H-3</th>\n",
       "      <th>CONF-3</th>\n",
       "      <th>TRANS-3</th>\n",
       "      <th>REV-3</th>\n",
       "      <th>FnG-3</th>\n",
       "      <th>Variation-4</th>\n",
       "      <th>Vol-4</th>\n",
       "      <th>RSI-4</th>\n",
       "      <th>MACD-4</th>\n",
       "      <th>MACD_H-4</th>\n",
       "      <th>CONF-4</th>\n",
       "      <th>TRANS-4</th>\n",
       "      <th>REV-4</th>\n",
       "      <th>FnG-4</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>-0.326118</td>\n",
       "      <td>-4.184569</td>\n",
       "      <td>-1.814295</td>\n",
       "      <td>-1.479953</td>\n",
       "      <td>-1.057732</td>\n",
       "      <td>1.828350</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.462483</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-1.420727</td>\n",
       "      <td>-2.584947</td>\n",
       "      <td>-0.437581</td>\n",
       "      <td>-1.626830</td>\n",
       "      <td>-1.320379</td>\n",
       "      <td>-0.741794</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-0.847688</td>\n",
       "      <td>0.955604</td>\n",
       "      <td>-0.427993</td>\n",
       "      <td>-1.632900</td>\n",
       "      <td>-1.235294</td>\n",
       "      <td>-0.618233</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-0.142410</td>\n",
       "      <td>-1.011481</td>\n",
       "      <td>-0.152676</td>\n",
       "      <td>-1.204172</td>\n",
       "      <td>-1.202571</td>\n",
       "      <td>-0.654534</td>\n",
       "      <td>1.144750</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-1.244407</td>\n",
       "      <td>-2.712213</td>\n",
       "      <td>-0.292146</td>\n",
       "      <td>-1.043523</td>\n",
       "      <td>-1.108359</td>\n",
       "      <td>-0.475907</td>\n",
       "      <td>1.144750</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-0.583209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-07</td>\n",
       "      <td>-0.331965</td>\n",
       "      <td>-0.495589</td>\n",
       "      <td>-1.457407</td>\n",
       "      <td>-1.552132</td>\n",
       "      <td>-0.860381</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>1.072032</td>\n",
       "      <td>0.525668</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>-0.318730</td>\n",
       "      <td>2.918547</td>\n",
       "      <td>-0.088527</td>\n",
       "      <td>-1.212491</td>\n",
       "      <td>-1.523793</td>\n",
       "      <td>-0.973190</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-1.552966</td>\n",
       "      <td>-4.184569</td>\n",
       "      <td>-0.326118</td>\n",
       "      <td>-1.814295</td>\n",
       "      <td>-1.479953</td>\n",
       "      <td>-1.057732</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-1.420727</td>\n",
       "      <td>-2.584947</td>\n",
       "      <td>-0.437581</td>\n",
       "      <td>-1.626830</td>\n",
       "      <td>-1.320379</td>\n",
       "      <td>-0.741794</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-0.847688</td>\n",
       "      <td>0.955604</td>\n",
       "      <td>-0.427993</td>\n",
       "      <td>-1.632900</td>\n",
       "      <td>-1.235294</td>\n",
       "      <td>-0.618233</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-0.774692</td>\n",
       "      <td>-0.186892</td>\n",
       "      <td>-0.142410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>-0.452131</td>\n",
       "      <td>1.422827</td>\n",
       "      <td>-0.830140</td>\n",
       "      <td>-1.412981</td>\n",
       "      <td>-0.171285</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>0.054294</td>\n",
       "      <td>0.626783</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>0.033910</td>\n",
       "      <td>2.134561</td>\n",
       "      <td>-0.323021</td>\n",
       "      <td>-1.063630</td>\n",
       "      <td>-1.504877</td>\n",
       "      <td>-0.549827</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>-0.583209</td>\n",
       "      <td>-0.495589</td>\n",
       "      <td>-0.331965</td>\n",
       "      <td>-1.457407</td>\n",
       "      <td>-1.552132</td>\n",
       "      <td>-0.860381</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>-0.318730</td>\n",
       "      <td>2.918547</td>\n",
       "      <td>-0.088527</td>\n",
       "      <td>-1.212491</td>\n",
       "      <td>-1.523793</td>\n",
       "      <td>-0.973190</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-1.552966</td>\n",
       "      <td>-4.184569</td>\n",
       "      <td>-0.326118</td>\n",
       "      <td>-1.814295</td>\n",
       "      <td>-1.479953</td>\n",
       "      <td>-1.057732</td>\n",
       "      <td>1.114249</td>\n",
       "      <td>-1.144752</td>\n",
       "      <td>-0.264496</td>\n",
       "      <td>-1.420727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-02-11</td>\n",
       "      <td>-0.485525</td>\n",
       "      <td>-1.529055</td>\n",
       "      <td>-1.178450</td>\n",
       "      <td>-1.298888</td>\n",
       "      <td>0.178688</td>\n",
       "      <td>0.330456</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.655642</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-2.453448</td>\n",
       "      <td>-0.455877</td>\n",
       "      <td>-0.539129</td>\n",
       "      <td>-0.393589</td>\n",
       "      <td>-0.401930</td>\n",
       "      <td>-0.947564</td>\n",
       "      <td>-1.334806</td>\n",
       "      <td>0.091563</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-2.453448</td>\n",
       "      <td>-0.455877</td>\n",
       "      <td>0.474709</td>\n",
       "      <td>1.422827</td>\n",
       "      <td>-0.452131</td>\n",
       "      <td>-0.830140</td>\n",
       "      <td>-1.412981</td>\n",
       "      <td>-0.171285</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>0.033910</td>\n",
       "      <td>2.134561</td>\n",
       "      <td>-0.323021</td>\n",
       "      <td>-1.063630</td>\n",
       "      <td>-1.504877</td>\n",
       "      <td>-0.549827</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>-0.583209</td>\n",
       "      <td>-0.495589</td>\n",
       "      <td>-0.331965</td>\n",
       "      <td>-1.457407</td>\n",
       "      <td>-1.552132</td>\n",
       "      <td>-0.860381</td>\n",
       "      <td>0.453384</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>-0.318730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>-0.506967</td>\n",
       "      <td>-1.004265</td>\n",
       "      <td>-0.584168</td>\n",
       "      <td>-1.121564</td>\n",
       "      <td>0.570097</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.722799</td>\n",
       "      <td>0.276565</td>\n",
       "      <td>-1.522212</td>\n",
       "      <td>-0.160496</td>\n",
       "      <td>-0.362809</td>\n",
       "      <td>2.482440</td>\n",
       "      <td>-0.478760</td>\n",
       "      <td>-0.746790</td>\n",
       "      <td>-1.191801</td>\n",
       "      <td>0.455801</td>\n",
       "      <td>0.276565</td>\n",
       "      <td>-2.453448</td>\n",
       "      <td>-0.455877</td>\n",
       "      <td>-0.054250</td>\n",
       "      <td>-1.529055</td>\n",
       "      <td>-0.485525</td>\n",
       "      <td>-1.178450</td>\n",
       "      <td>-1.298888</td>\n",
       "      <td>0.178688</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-2.453448</td>\n",
       "      <td>-0.455877</td>\n",
       "      <td>-0.539129</td>\n",
       "      <td>-0.393589</td>\n",
       "      <td>-0.401930</td>\n",
       "      <td>-0.947564</td>\n",
       "      <td>-1.334806</td>\n",
       "      <td>0.091563</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-2.453448</td>\n",
       "      <td>-0.455877</td>\n",
       "      <td>0.474709</td>\n",
       "      <td>1.422827</td>\n",
       "      <td>-0.452131</td>\n",
       "      <td>-0.830140</td>\n",
       "      <td>-1.412981</td>\n",
       "      <td>-0.171285</td>\n",
       "      <td>0.207605</td>\n",
       "      <td>-1.959889</td>\n",
       "      <td>-0.596734</td>\n",
       "      <td>0.033910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>2022-08-28</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>-0.601366</td>\n",
       "      <td>-2.100264</td>\n",
       "      <td>-0.799658</td>\n",
       "      <td>-1.290428</td>\n",
       "      <td>0.181253</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>-0.074029</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.671369</td>\n",
       "      <td>-0.331552</td>\n",
       "      <td>0.723698</td>\n",
       "      <td>-2.049936</td>\n",
       "      <td>-0.689661</td>\n",
       "      <td>-1.213271</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-0.310257</td>\n",
       "      <td>-0.097684</td>\n",
       "      <td>-0.671369</td>\n",
       "      <td>-1.658930</td>\n",
       "      <td>1.339033</td>\n",
       "      <td>-2.000380</td>\n",
       "      <td>-0.584031</td>\n",
       "      <td>-1.132733</td>\n",
       "      <td>-1.025714</td>\n",
       "      <td>-0.310257</td>\n",
       "      <td>-0.097684</td>\n",
       "      <td>-0.715449</td>\n",
       "      <td>0.196202</td>\n",
       "      <td>0.769656</td>\n",
       "      <td>-1.385521</td>\n",
       "      <td>-0.464083</td>\n",
       "      <td>-0.979903</td>\n",
       "      <td>-1.025714</td>\n",
       "      <td>-0.310257</td>\n",
       "      <td>-0.097684</td>\n",
       "      <td>-0.803608</td>\n",
       "      <td>-0.212717</td>\n",
       "      <td>0.816704</td>\n",
       "      <td>-1.545836</td>\n",
       "      <td>-0.444016</td>\n",
       "      <td>-1.152727</td>\n",
       "      <td>-1.025714</td>\n",
       "      <td>-0.498097</td>\n",
       "      <td>-0.206563</td>\n",
       "      <td>-0.803608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>0.943757</td>\n",
       "      <td>-0.691652</td>\n",
       "      <td>-1.653641</td>\n",
       "      <td>-0.871357</td>\n",
       "      <td>-1.020465</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>0.069068</td>\n",
       "      <td>0.238574</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.715449</td>\n",
       "      <td>0.845674</td>\n",
       "      <td>0.850751</td>\n",
       "      <td>-1.597787</td>\n",
       "      <td>-0.821959</td>\n",
       "      <td>-1.096574</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.847688</td>\n",
       "      <td>-0.601366</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>-2.100264</td>\n",
       "      <td>-0.799658</td>\n",
       "      <td>-1.290428</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.671369</td>\n",
       "      <td>-0.331552</td>\n",
       "      <td>0.723698</td>\n",
       "      <td>-2.049936</td>\n",
       "      <td>-0.689661</td>\n",
       "      <td>-1.213271</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-0.310257</td>\n",
       "      <td>-0.097684</td>\n",
       "      <td>-0.671369</td>\n",
       "      <td>-1.658930</td>\n",
       "      <td>1.339033</td>\n",
       "      <td>-2.000380</td>\n",
       "      <td>-0.584031</td>\n",
       "      <td>-1.132733</td>\n",
       "      <td>-1.025714</td>\n",
       "      <td>-0.310257</td>\n",
       "      <td>-0.097684</td>\n",
       "      <td>-0.715449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>0.726988</td>\n",
       "      <td>0.047069</td>\n",
       "      <td>-1.336325</td>\n",
       "      <td>-0.871305</td>\n",
       "      <td>-0.646449</td>\n",
       "      <td>-0.029558</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.612060</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-1.024008</td>\n",
       "      <td>0.277823</td>\n",
       "      <td>0.880353</td>\n",
       "      <td>-1.406081</td>\n",
       "      <td>-0.880154</td>\n",
       "      <td>-0.841249</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-0.891768</td>\n",
       "      <td>-0.691652</td>\n",
       "      <td>0.943757</td>\n",
       "      <td>-1.653641</td>\n",
       "      <td>-0.871357</td>\n",
       "      <td>-1.020465</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.715449</td>\n",
       "      <td>0.845674</td>\n",
       "      <td>0.850751</td>\n",
       "      <td>-1.597787</td>\n",
       "      <td>-0.821959</td>\n",
       "      <td>-1.096574</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.847688</td>\n",
       "      <td>-0.601366</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>-2.100264</td>\n",
       "      <td>-0.799658</td>\n",
       "      <td>-1.290428</td>\n",
       "      <td>-1.263537</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.671369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>2022-09-03</td>\n",
       "      <td>0.395940</td>\n",
       "      <td>-0.231177</td>\n",
       "      <td>-0.964656</td>\n",
       "      <td>-0.866931</td>\n",
       "      <td>-0.401653</td>\n",
       "      <td>-0.077619</td>\n",
       "      <td>-0.323408</td>\n",
       "      <td>0.610305</td>\n",
       "      <td>-1.063067</td>\n",
       "      <td>-0.648298</td>\n",
       "      <td>-0.307825</td>\n",
       "      <td>-0.979928</td>\n",
       "      <td>-0.255088</td>\n",
       "      <td>0.673668</td>\n",
       "      <td>-0.707947</td>\n",
       "      <td>-0.867803</td>\n",
       "      <td>-0.506193</td>\n",
       "      <td>-1.063067</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-0.803608</td>\n",
       "      <td>0.047069</td>\n",
       "      <td>0.726988</td>\n",
       "      <td>-1.336325</td>\n",
       "      <td>-0.871305</td>\n",
       "      <td>-0.646449</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-1.024008</td>\n",
       "      <td>0.277823</td>\n",
       "      <td>0.880353</td>\n",
       "      <td>-1.406081</td>\n",
       "      <td>-0.880154</td>\n",
       "      <td>-0.841249</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-0.891768</td>\n",
       "      <td>-0.691652</td>\n",
       "      <td>0.943757</td>\n",
       "      <td>-1.653641</td>\n",
       "      <td>-0.871357</td>\n",
       "      <td>-1.020465</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-1.414496</td>\n",
       "      <td>-0.174982</td>\n",
       "      <td>-0.715449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>0.658018</td>\n",
       "      <td>-0.278482</td>\n",
       "      <td>-1.133325</td>\n",
       "      <td>-0.831832</td>\n",
       "      <td>-0.166540</td>\n",
       "      <td>-0.269198</td>\n",
       "      <td>-0.256709</td>\n",
       "      <td>0.691814</td>\n",
       "      <td>-0.742360</td>\n",
       "      <td>-0.648298</td>\n",
       "      <td>-0.307825</td>\n",
       "      <td>-0.891768</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>0.478227</td>\n",
       "      <td>-1.113287</td>\n",
       "      <td>-0.844382</td>\n",
       "      <td>-0.254845</td>\n",
       "      <td>-1.063067</td>\n",
       "      <td>-0.648298</td>\n",
       "      <td>-0.307825</td>\n",
       "      <td>-1.024008</td>\n",
       "      <td>-0.231177</td>\n",
       "      <td>0.395940</td>\n",
       "      <td>-0.964656</td>\n",
       "      <td>-0.866931</td>\n",
       "      <td>-0.401653</td>\n",
       "      <td>-1.063067</td>\n",
       "      <td>-0.648298</td>\n",
       "      <td>-0.307825</td>\n",
       "      <td>-0.979928</td>\n",
       "      <td>-0.255088</td>\n",
       "      <td>0.673668</td>\n",
       "      <td>-0.707947</td>\n",
       "      <td>-0.867803</td>\n",
       "      <td>-0.506193</td>\n",
       "      <td>-1.063067</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-0.803608</td>\n",
       "      <td>0.047069</td>\n",
       "      <td>0.726988</td>\n",
       "      <td>-1.336325</td>\n",
       "      <td>-0.871305</td>\n",
       "      <td>-0.646449</td>\n",
       "      <td>-1.313931</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.277036</td>\n",
       "      <td>-1.024008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Volume  Variation  ...     REV-4     FnG-4  Target\n",
       "0     2018-02-05 -0.326118  -4.184569  ... -0.186892 -0.583209       1\n",
       "2     2018-02-07 -0.331965  -0.495589  ... -0.186892 -0.142410       1\n",
       "4     2018-02-09 -0.452131   1.422827  ... -0.264496 -1.420727       0\n",
       "6     2018-02-11 -0.485525  -1.529055  ... -0.596734 -0.318730       1\n",
       "8     2018-02-13 -0.506967  -1.004265  ... -0.596734  0.033910       1\n",
       "...          ...       ...        ...  ...       ...       ...     ...\n",
       "1668  2022-08-28  0.433926  -0.601366  ... -0.206563 -0.803608       1\n",
       "1670  2022-08-30  0.943757  -0.691652  ... -0.097684 -0.715449       1\n",
       "1672  2022-09-01  0.726988   0.047069  ... -0.174982 -0.671369       0\n",
       "1674  2022-09-03  0.395940  -0.231177  ... -0.174982 -0.715449       0\n",
       "1676  2022-09-05  0.658018  -0.278482  ... -0.277036 -1.024008       0\n",
       "\n",
       "[839 rows x 50 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr = predict_data.corr(\"pearson\")\n",
    "#corr[[\"Target\"]].to_clipboard()\n",
    "#corr[[\"Target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mode = \"RAND\"\n",
    "\n",
    "if split_mode == \"RAND\":\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(predict_data.drop([\"Date\",\"Target\"],axis=1), predict_data[\"Target\"], test_size=0.33, random_state=100)\n",
    "elif split_mode == \"STATIC\":\n",
    "    train_data = predict_data[0:int(0.66*len(predict_data))].drop([\"Date\",\"Target\"],axis=1)\n",
    "    train_labels = predict_data[0:int(0.66*len(predict_data))][\"Target\"]\n",
    "    test_data = predict_data[int(0.66*len(predict_data)):len(predict_data)].drop([\"Date\",\"Target\"],axis=1)\n",
    "    test_labels = predict_data[int(0.66*len(predict_data)):len(predict_data)][\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75101587\n",
      "Iteration 2, loss = 0.69023775\n",
      "Iteration 3, loss = 0.66252296\n",
      "Iteration 4, loss = 0.64346398\n",
      "Iteration 5, loss = 0.62759941\n",
      "Iteration 6, loss = 0.60726810\n",
      "Iteration 7, loss = 0.58357481\n",
      "Iteration 8, loss = 0.56351119\n",
      "Iteration 9, loss = 0.54456601\n",
      "Iteration 10, loss = 0.52373617\n",
      "Iteration 11, loss = 0.47815434\n",
      "Iteration 12, loss = 0.45968312\n",
      "Iteration 13, loss = 0.42683962\n",
      "Iteration 14, loss = 0.39388749\n",
      "Iteration 15, loss = 0.35376035\n",
      "Iteration 16, loss = 0.32010218\n",
      "Iteration 17, loss = 0.28718691\n",
      "Iteration 18, loss = 0.25470753\n",
      "Iteration 19, loss = 0.23305351\n",
      "Iteration 20, loss = 0.19373705\n",
      "Iteration 21, loss = 0.18939781\n",
      "Iteration 22, loss = 0.22755299\n",
      "Iteration 23, loss = 0.15900613\n",
      "Iteration 24, loss = 0.13473910\n",
      "Iteration 25, loss = 0.12978465\n",
      "Iteration 26, loss = 0.12690233\n",
      "Iteration 27, loss = 0.11534901\n",
      "Iteration 28, loss = 0.08571650\n",
      "Iteration 29, loss = 0.07297928\n",
      "Iteration 30, loss = 0.05928956\n",
      "Iteration 31, loss = 0.04919931\n",
      "Iteration 32, loss = 0.04554460\n",
      "Iteration 33, loss = 0.03973692\n",
      "Iteration 34, loss = 0.03189002\n",
      "Iteration 35, loss = 0.02655386\n",
      "Iteration 36, loss = 0.02116080\n",
      "Iteration 37, loss = 0.02162353\n",
      "Iteration 38, loss = 0.01757151\n",
      "Iteration 39, loss = 0.01551302\n",
      "Iteration 40, loss = 0.01398643\n",
      "Iteration 41, loss = 0.01119291\n",
      "Iteration 42, loss = 0.00948874\n",
      "Iteration 43, loss = 0.00829736\n",
      "Iteration 44, loss = 0.00751358\n",
      "Iteration 45, loss = 0.00677385\n",
      "Iteration 46, loss = 0.00621194\n",
      "Iteration 47, loss = 0.00575584\n",
      "Iteration 48, loss = 0.00534890\n",
      "Iteration 49, loss = 0.00495596\n",
      "Iteration 50, loss = 0.00467117\n",
      "Iteration 51, loss = 0.00436846\n",
      "Iteration 52, loss = 0.00410222\n",
      "Iteration 53, loss = 0.00389865\n",
      "Iteration 54, loss = 0.00367897\n",
      "Iteration 55, loss = 0.00353677\n",
      "Iteration 56, loss = 0.00333372\n",
      "Iteration 57, loss = 0.00318838\n",
      "Iteration 58, loss = 0.00306909\n",
      "Iteration 59, loss = 0.00294663\n",
      "Iteration 60, loss = 0.00282577\n",
      "Iteration 61, loss = 0.00272320\n",
      "Iteration 62, loss = 0.00261723\n",
      "Iteration 63, loss = 0.00253677\n",
      "Iteration 64, loss = 0.00241320\n",
      "Iteration 65, loss = 0.00236667\n",
      "Iteration 66, loss = 0.00226897\n",
      "Iteration 67, loss = 0.00218446\n",
      "Iteration 68, loss = 0.00214369\n",
      "Iteration 69, loss = 0.00206416\n",
      "Iteration 70, loss = 0.00199627\n",
      "Iteration 71, loss = 0.00194572\n",
      "Iteration 72, loss = 0.00188667\n",
      "Iteration 73, loss = 0.00183690\n",
      "Iteration 74, loss = 0.00178588\n",
      "Iteration 75, loss = 0.00174537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(200, 2000, 200), verbose=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = \"MLP\"\n",
    "\n",
    "if algo == \"MLP\":\n",
    "    model = MLPClassifier(hidden_layer_sizes=(200,2000,200),verbose=True)\n",
    "elif algo == \"RF\":\n",
    "    model = RandomForestClassifier(n_estimators=100,verbose=False)\n",
    "elif algo == \"CAT\":\n",
    "    model = CatBoostClassifier(iterations=100,depth=12)\n",
    "elif algo == \"TREE\":\n",
    "    model = DecisionTreeClassifier(splitter=\"random\")\n",
    "model.fit(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5018050541516246"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53, 72],\n",
       "       [66, 86]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(test_labels,preds)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44537815126050423"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN = conf[0,0] / (conf[0,0] + conf[1,0])\n",
    "TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5443037974683544"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = conf[1,1] / (conf[1,1] + conf[0,1])\n",
    "TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(predict_data, model_name):\n",
    "    scores = []\n",
    "    TPs = []\n",
    "    TNs = []\n",
    "    for randomize in range(1,20):\n",
    "        train_data, test_data, train_labels, test_labels = train_test_split(predict_data.drop([\"Date\",\"Target\"],axis=1), predict_data[\"Target\"], test_size=0.33, random_state=randomize)\n",
    "        if model_name == \"MLP\":\n",
    "            model = MLPClassifier(hidden_layer_sizes=(200,2000,200))\n",
    "        elif model_name == \"CAT\":\n",
    "            model = CatBoostClassifier(iterations=100,depth=12,verbose=False)\n",
    "        elif model_name == \"RF\":\n",
    "            model = RandomForestClassifier(n_estimators=100,verbose=False)\n",
    "        model.fit(train_data, train_labels)\n",
    "        scores.append(model.score(test_data, test_labels))\n",
    "        conf = confusion_matrix(test_labels,model.predict(test_data))\n",
    "        TN = conf[0,0] / (conf[0,0] + conf[1,0])\n",
    "        TP = conf[1,1] / (conf[1,1] + conf[0,1])\n",
    "        TPs.append(TP)\n",
    "        TNs.append(TN)\n",
    "        print(\"Score : {} || TP : {} || TN : {}\".format(scores[-1],TP,TN))\n",
    "\n",
    "\n",
    "    return scores,TPs,TNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores,TPs,TNs = cross_validation_loop(predict_data,\"CAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score moyen : 0.5067452023560707 || TP moyen : 0.5411620348700146 || TN moyen : 0.4619500842660272\n"
     ]
    }
   ],
   "source": [
    "print(\"CatBoost score moyen : {} || TP moyen : {} || TN moyen : {}\".format(mean(scores),mean(TPs),mean(TNs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.51985559566787 || TP : 0.5694444444444444 || TN : 0.46616541353383456\n",
      "Score : 0.49458483754512633 || TP : 0.5194805194805194 || TN : 0.4634146341463415\n",
      "Score : 0.5054151624548736 || TP : 0.5465116279069767 || TN : 0.4380952380952381\n",
      "Score : 0.48375451263537905 || TP : 0.5136986301369864 || TN : 0.45038167938931295\n",
      "Score : 0.4693140794223827 || TP : 0.4722222222222222 || TN : 0.4639175257731959\n",
      "Score : 0.48736462093862815 || TP : 0.525 || TN : 0.4358974358974359\n",
      "Score : 0.51985559566787 || TP : 0.5280898876404494 || TN : 0.5050505050505051\n",
      "Score : 0.49458483754512633 || TP : 0.5611510791366906 || TN : 0.427536231884058\n",
      "Score : 0.49458483754512633 || TP : 0.5384615384615384 || TN : 0.4380165289256198\n",
      "Score : 0.4657039711191336 || TP : 0.4899328859060403 || TN : 0.4375\n",
      "Score : 0.4693140794223827 || TP : 0.5205479452054794 || TN : 0.4122137404580153\n",
      "Score : 0.5306859205776173 || TP : 0.5714285714285714 || TN : 0.46788990825688076\n",
      "Score : 0.4584837545126354 || TP : 0.5 || TN : 0.408\n",
      "Score : 0.5379061371841155 || TP : 0.5757575757575758 || TN : 0.48214285714285715\n",
      "Score : 0.5090252707581228 || TP : 0.5054945054945055 || TN : 0.5157894736842106\n",
      "Score : 0.5342960288808665 || TP : 0.5511363636363636 || TN : 0.504950495049505\n",
      "Score : 0.5487364620938628 || TP : 0.6050955414012739 || TN : 0.475\n",
      "Score : 0.5270758122743683 || TP : 0.567741935483871 || TN : 0.47540983606557374\n",
      "Score : 0.5270758122743683 || TP : 0.5636363636363636 || TN : 0.4732142857142857\n"
     ]
    }
   ],
   "source": [
    "scores,TPs,TNs = cross_validation_loop(predict_data,\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest score moyen : 0.5040851225536767 || TP moyen : 0.538149033546309 || TN moyen : 0.4600308310035195\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest score moyen : {} || TP moyen : {} || TN moyen : {}\".format(mean(scores),mean(TPs),mean(TNs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.51985559566787 || TP : 0.5675675675675675 || TN : 0.46511627906976744\n",
      "Score : 0.48014440433212996 || TP : 0.5068493150684932 || TN : 0.45038167938931295\n",
      "Score : 0.5126353790613718 || TP : 0.56 || TN : 0.4566929133858268\n",
      "Score : 0.5234657039711191 || TP : 0.5496688741721855 || TN : 0.49206349206349204\n",
      "Score : 0.4657039711191336 || TP : 0.46308724832214765 || TN : 0.46875\n",
      "Score : 0.5342960288808665 || TP : 0.5734265734265734 || TN : 0.4925373134328358\n",
      "Score : 0.5270758122743683 || TP : 0.541095890410959 || TN : 0.5114503816793893\n",
      "Score : 0.5054151624548736 || TP : 0.5657894736842105 || TN : 0.432\n",
      "Score : 0.48014440433212996 || TP : 0.5266666666666666 || TN : 0.4251968503937008\n",
      "Score : 0.5090252707581228 || TP : 0.5302013422818792 || TN : 0.484375\n",
      "Score : 0.5126353790613718 || TP : 0.56 || TN : 0.4566929133858268\n",
      "Score : 0.5451263537906137 || TP : 0.5933333333333334 || TN : 0.4881889763779528\n",
      "Score : 0.4729241877256318 || TP : 0.5131578947368421 || TN : 0.424\n",
      "Score : 0.5342960288808665 || TP : 0.5845070422535211 || TN : 0.48148148148148145\n",
      "Score : 0.5018050541516246 || TP : 0.5 || TN : 0.5037593984962406\n",
      "Score : 0.51985559566787 || TP : 0.5460526315789473 || TN : 0.488\n",
      "Score : 0.5054151624548736 || TP : 0.5695364238410596 || TN : 0.42857142857142855\n",
      "Score : 0.48014440433212996 || TP : 0.5256410256410257 || TN : 0.4214876033057851\n",
      "Score : 0.51985559566787 || TP : 0.5664335664335665 || TN : 0.4701492537313433\n"
     ]
    }
   ],
   "source": [
    "scores,TPs,TNs = cross_validation_loop(predict_data,\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP score moyen : 0.5078852365570967 || TP moyen : 0.5443692036536304 || TN moyen : 0.4653102613033886\n"
     ]
    }
   ],
   "source": [
    "print(\"MLP score moyen : {} || TP moyen : {} || TN moyen : {}\".format(mean(scores),mean(TPs),mean(TNs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (len(predict_data.columns)-2,)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=shape),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(2000, activation='relu'),\n",
    "    tf.keras.layers.Dense(2000, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562, 48) (277, 48)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 4s 142ms/step - loss: 0.7538 - accuracy: 0.5320 - precision: 0.5391 - recall: 0.8087 - val_loss: 0.6873 - val_accuracy: 0.5487 - val_precision: 0.5487 - val_recall: 1.0000\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.6805 - accuracy: 0.5302 - precision: 0.5304 - recall: 0.9966 - val_loss: 0.6878 - val_accuracy: 0.5668 - val_precision: 0.5769 - val_recall: 0.7895\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.6691 - accuracy: 0.5765 - precision: 0.5743 - recall: 0.7785 - val_loss: 0.6927 - val_accuracy: 0.5307 - val_precision: 0.5775 - val_recall: 0.5395\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.6651 - accuracy: 0.6139 - precision: 0.6421 - recall: 0.6141 - val_loss: 0.7008 - val_accuracy: 0.5523 - val_precision: 0.5854 - val_recall: 0.6316\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 2s 104ms/step - loss: 0.6455 - accuracy: 0.6423 - precision: 0.6667 - recall: 0.6510 - val_loss: 0.7416 - val_accuracy: 0.5126 - val_precision: 0.5669 - val_recall: 0.4737\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.6043 - accuracy: 0.6708 - precision: 0.7116 - recall: 0.6376 - val_loss: 0.7345 - val_accuracy: 0.5054 - val_precision: 0.5789 - val_recall: 0.3618\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 0.5761 - accuracy: 0.6851 - precision: 0.7336 - recall: 0.6376 - val_loss: 0.7907 - val_accuracy: 0.4838 - val_precision: 0.5344 - val_recall: 0.4605\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 2s 93ms/step - loss: 0.5255 - accuracy: 0.7367 - precision: 0.7757 - recall: 0.7081 - val_loss: 0.8596 - val_accuracy: 0.4621 - val_precision: 0.5190 - val_recall: 0.2697\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 2s 95ms/step - loss: 0.4910 - accuracy: 0.7544 - precision: 0.8030 - recall: 0.7114 - val_loss: 0.9029 - val_accuracy: 0.4838 - val_precision: 0.5287 - val_recall: 0.5461\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 0.4297 - accuracy: 0.7989 - precision: 0.8339 - recall: 0.7752 - val_loss: 1.0199 - val_accuracy: 0.4910 - val_precision: 0.5337 - val_recall: 0.5724\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 0.3645 - accuracy: 0.8167 - precision: 0.8545 - recall: 0.7886 - val_loss: 1.4128 - val_accuracy: 0.5126 - val_precision: 0.5399 - val_recall: 0.7566\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 2s 101ms/step - loss: 0.2917 - accuracy: 0.8719 - precision: 0.8844 - recall: 0.8725 - val_loss: 1.2036 - val_accuracy: 0.5090 - val_precision: 0.5513 - val_recall: 0.5658\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 2s 102ms/step - loss: 0.2051 - accuracy: 0.9128 - precision: 0.9338 - recall: 0.8993 - val_loss: 1.6257 - val_accuracy: 0.4765 - val_precision: 0.5200 - val_recall: 0.5987\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 2s 105ms/step - loss: 0.1910 - accuracy: 0.9181 - precision: 0.9145 - recall: 0.9329 - val_loss: 2.0143 - val_accuracy: 0.4765 - val_precision: 0.5207 - val_recall: 0.5789\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 2s 96ms/step - loss: 0.1838 - accuracy: 0.9164 - precision: 0.9254 - recall: 0.9161 - val_loss: 1.5950 - val_accuracy: 0.4838 - val_precision: 0.5319 - val_recall: 0.4934\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 2s 98ms/step - loss: 0.1378 - accuracy: 0.9466 - precision: 0.9497 - recall: 0.9497 - val_loss: 1.7479 - val_accuracy: 0.5379 - val_precision: 0.6017 - val_recall: 0.4671\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 0.1407 - accuracy: 0.9520 - precision: 0.9532 - recall: 0.9564 - val_loss: 2.1295 - val_accuracy: 0.4874 - val_precision: 0.5325 - val_recall: 0.5395\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 2s 122ms/step - loss: 0.1156 - accuracy: 0.9484 - precision: 0.9559 - recall: 0.9463 - val_loss: 2.1885 - val_accuracy: 0.4946 - val_precision: 0.5429 - val_recall: 0.5000\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 0.1015 - accuracy: 0.9591 - precision: 0.9568 - recall: 0.9664 - val_loss: 2.3965 - val_accuracy: 0.4765 - val_precision: 0.5248 - val_recall: 0.4868\n",
      "Epoch 20/30\n",
      "18/18 [==============================] - 2s 137ms/step - loss: 0.1526 - accuracy: 0.9448 - precision: 0.9495 - recall: 0.9463 - val_loss: 1.8993 - val_accuracy: 0.4982 - val_precision: 0.5430 - val_recall: 0.5395oss: 0.1529 - accuracy: 0.9554 - precision: 0.9496 - rec\n",
      "Epoch 21/30\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.1521 - accuracy: 0.9466 - precision: 0.9653 - recall: 0.9329 - val_loss: 1.9083 - val_accuracy: 0.5199 - val_precision: 0.5583 - val_recall: 0.5987\n",
      "Epoch 22/30\n",
      "18/18 [==============================] - 2s 141ms/step - loss: 0.1349 - accuracy: 0.9502 - precision: 0.9470 - recall: 0.9597 - val_loss: 2.0300 - val_accuracy: 0.5126 - val_precision: 0.5548 - val_recall: 0.5658\n",
      "Epoch 23/30\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.0826 - accuracy: 0.9751 - precision: 0.9797 - recall: 0.9732 - val_loss: 2.0398 - val_accuracy: 0.5054 - val_precision: 0.5532 - val_recall: 0.5132\n",
      "Epoch 24/30\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 0.0507 - accuracy: 0.9840 - precision: 0.9833 - recall: 0.9866 - val_loss: 2.4049 - val_accuracy: 0.4838 - val_precision: 0.5328 - val_recall: 0.4803\n",
      "Epoch 25/30\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 0.0375 - accuracy: 0.9840 - precision: 0.9865 - recall: 0.9832 - val_loss: 2.3831 - val_accuracy: 0.5090 - val_precision: 0.5678 - val_recall: 0.4408\n",
      "Epoch 26/30\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.0234 - accuracy: 0.9929 - precision: 0.9900 - recall: 0.9966 - val_loss: 2.5314 - val_accuracy: 0.5054 - val_precision: 0.5547 - val_recall: 0.5000\n",
      "Epoch 27/30\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0126 - accuracy: 0.9964 - precision: 0.9966 - recall: 0.9966 - val_loss: 2.7888 - val_accuracy: 0.4910 - val_precision: 0.5314 - val_recall: 0.6118\n",
      "Epoch 28/30\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.0047 - accuracy: 0.9982 - precision: 1.0000 - recall: 0.9966 - val_loss: 2.9194 - val_accuracy: 0.4765 - val_precision: 0.5223 - val_recall: 0.5395\n",
      "Epoch 29/30\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.0061 - accuracy: 0.9982 - precision: 0.9967 - recall: 1.0000 - val_loss: 2.9351 - val_accuracy: 0.4946 - val_precision: 0.5423 - val_recall: 0.5066\n",
      "Epoch 30/30\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.0066 - accuracy: 0.9964 - precision: 0.9966 - recall: 0.9966 - val_loss: 3.0533 - val_accuracy: 0.5090 - val_precision: 0.5526 - val_recall: 0.5526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2631d130d90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=30, validation_data=(test_data,test_labels),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 3.1906 - accuracy: 0.4414 - precision: 0.5116 - recall: 0.3492 - 28ms/epoch - 7ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7593e2cc95d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTest accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(probability_model.predict(test_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51, 74],\n",
       "       [62, 90]], dtype=int64)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(test_labels,preds)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45132743362831856"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN = conf[0,0] / (conf[0,0] + conf[1,0])\n",
    "TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5487804878048781"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = conf[1,1] / (conf[1,1] + conf[0,1])\n",
    "TP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d62b7309829161c9ff8c8cb2799597e552c804f98ee796508f623761c3c8a87d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
